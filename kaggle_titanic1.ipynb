{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# almost certainly will need these imported\nimport pandas as pd\nimport numpy as np\nimport xgboost as xg\n\n# sklearn stuff\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import precision_recall_curve, f1_score, auc, accuracy_score, log_loss, classification_report,confusion_matrix,roc_curve,roc_auc_score\n\n# will be doing some optimization I'm sure\nimport hyperopt\nfrom hyperopt import STATUS_OK, Trials, fmin, hp, tpe","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv(\"train.csv\")\ntest_data = pd.read_csv(\"test.csv\")\nsample_result = pd.read_csv(\"gender_submission.csv\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.head()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# how many NaN's by column?\n\nfor column in train_data.columns:\n    print(\"variable:\", column, \"NaN count:\", train_data[column].isna().sum())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Well, with 687 NaN's out of 891 rows (77%) for 'Cabin' it seems reasonable to just drop that column.  With only 2 NaN's for 'Embarked' I'm going to just drop the NaN's.  The really problematic variable is 'Age' - which, from a common sense perspective, seems likely to be relevant to the outcome, and has just enough NaN's to be problematic.  Also I'm skeptical that these values can be reasonably imputed from the other variables.\n\nBut for now, let's drop 'Cabin' and the rows with NaN's for 'Embarked'.","metadata":{}},{"cell_type":"code","source":"train_data.drop('Cabin', axis=1, inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.columns","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"indexdrop = train_data[train_data['Embarked'].isna()].index","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.drop(indexdrop, inplace=True)\ntrain_data.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# what do the sample results look like?\nsample_result.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# and the test set?\ntest_data.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# kaggle sample code\n\"\"\"\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X, y)\npredictions = model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('submission.csv', index=False)\n\"\"\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# output.head()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ok, well, so what?  compare some models, maybe optimize some hyperparameters\n\nresults_cols = ['model type', 'hyperparameters', 'f1', 'roc_auc', 'accuracy']\nresults = pd.DataFrame(columns = results_cols)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# choosing all reasonable candidates for predictor\n# AGE REMAINS PROBLEMATIC WITH AROUND 20% NAN'S\n\n# predictor_variables = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n# can't run anything with NaN's, will omit variable as a first measure\n\npredictor_variables = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n\ny = train_data[\"Survived\"]\nX = train_data[predictor_variables]\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# encode variables that will require it\n# specifically \"Sex\" and \"Embarked\"\nX[[\"Sex\", \"Embarked\"]] = X[[\"Sex\", \"Embarked\"]].apply(LabelEncoder().fit_transform)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This appears to have worked, despite the above warning.","metadata":{}},{"cell_type":"code","source":"# split the train_data into train and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=133)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This gives us 711 observations for training, and 178 for validation.  Logistic regression seems like a good starting point, as good as any.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr = LogisticRegression()\nlr.fit(X_train, y_train)\ny_pred = lr.predict(X_val)\naccuracy = accuracy_score(y_val, y_pred)\nf1 = f1_score(y_val, y_pred)\nra = roc_auc_score(y_val, y_pred)\nitem = [\"Logistic regression\", \"N/A\", f1, ra, accuracy]\nitemdict = dict(zip(results_cols, item))\nresults=results.append(itemdict, ignore_index=True)\nprint(\"Logistic regression\\n\", \"Accuracy:\", accuracy, \"f1:\", f1, \"roc_auc:\", ra)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Well, I would hope for much better accuracy than that - but then, logistic regression is simple and quick.  Let's move on. (Also .concat doesn't seem to exist yet.)","metadata":{}},{"cell_type":"code","source":"results","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# random forest\n\nrfc = RandomForestClassifier(n_estimators=200, max_depth=5, random_state=20)\nrfc.fit(X_train, y_train)\ny_pred = rfc.predict(X_val)\naccuracy = accuracy_score(y_val, y_pred)\nf1 = f1_score(y_val, y_pred)\nra = roc_auc_score(y_val, y_pred)\nitem = [\"Random Forest\", \"n_estimators=200, max_depth=5\", f1, ra, accuracy]\nitemdict = dict(zip(results_cols, item))\nresults=results.append(itemdict, ignore_index=True)\nprint(\"Random Forest Classifier\\n\", \"Accuracy:\", accuracy, \"f1:\", f1, \"roc_auc:\", ra)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I suppose I should use hyperopt (or gridsearch) to choose hyperparameters; there's enough variability in the assessment metrics.","metadata":{}},{"cell_type":"code","source":"rf_space = {\"max_depth\": hp.quniform(\"max_depth\", 1, 8, 1),\n                \"n_estimators\": hp.quniform(\"n_estimators\", 80, 320, 20)}\n\ndef objective(rf_space):\n    rfc = RandomForestClassifier(\n        n_estimators=int(rf_space[\"n_estimators\"]),\n        max_depth=int(rf_space[\"max_depth\"]),\n        random_state=100)\n    \n    evaluation = [(X_train, y_train), (X_val, y_val)]\n    \n    rfc.fit(X_train, y_train)\n    \n    y_pred = rfc.predict(X_val)\n    accuracy = accuracy_score(y_val, y_pred)\n    f1 = f1_score(y_val, y_pred)\n    ra = roc_auc_score(y_val, y_pred)\n    return {'loss': -accuracy, 'accuracy': accuracy, 'f1': f1, 'roc_auc': ra, 'status': STATUS_OK}\n\ntrials = Trials()\n\nbest_hyperparams = hyperopt.fmin(fn = objective,\n                    space = rf_space,\n                    algo = hyperopt.tpe.suggest,\n                    max_evals = 100,\n                    trials = trials)\n    \nassess = trials.best_trial['result']\n\nitem = [\"Random Forest (Hyperopt)\", best_hyperparams, assess['f1'], assess['roc_auc'], assess['accuracy']]\nitemdict = dict(zip(results_cols, item))\nresults=results.append(itemdict, ignore_index=True)\nprint(\"Random Forest Classifier - Hyperopt\\n\", \"Hyperparameters:\", best_hyperparams, \"\\nAccuracy:\", accuracy, \"f1:\", f1, \"roc_auc:\", ra)    \n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results.tail()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This got accuracy up to 0.8539, a modest improvement.  I'm just going to optimize from the start for k-NN.","metadata":{}},{"cell_type":"code","source":"# k-Nearest Neighbors using hyperopt\n\nknn_space = {\"n_neighbors\": hp.quniform(\"n_neighbors\", 1, 12, 1),\n            \"weights\": hp.choice(\"weights\", [\"uniform\", \"distance\"])}\n\n\ndef objective(knn_space):\n    knn = KNeighborsClassifier(\n        n_neighbors=int(knn_space[\"n_neighbors\"]),\n        weights=knn_space[\"weights\"])\n        \n    evaluation = [(X_train, y_train), (X_val, y_val)]\n    \n    rfc.fit(X_train, y_train)\n    \n    y_pred = rfc.predict(X_val)\n    accuracy = accuracy_score(y_val, y_pred)\n    f1 = f1_score(y_val, y_pred)\n    ra = roc_auc_score(y_val, y_pred)\n    return {'loss': -accuracy, 'accuracy': accuracy, 'f1': f1, 'roc_auc': ra, 'status': STATUS_OK}\n\ntrials = Trials()\n\nbest_hyperparams = hyperopt.fmin(fn = objective,\n                    space = knn_space,\n                    algo = hyperopt.tpe.suggest,\n                    max_evals = 100,\n                    trials = trials)\n    \nassess = trials.best_trial['result']\n\nitem = [\"k-Nearest Neighbors (Hyperopt)\", best_hyperparams, assess['f1'], assess['roc_auc'], assess['accuracy']]\nitemdict = dict(zip(results_cols, item))\nresults=results.append(itemdict, ignore_index=True)\nprint(\"k-Nearest Neighbors (Hyperopt)\\n\", \"Hyperparameters:\", best_hyperparams, \"\\nAccuracy:\", accuracy, \"f1:\", f1, \"roc_auc:\", ra)    \n    \n\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results.tail()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I find it more than a bit odd that the assessment metrics for the RF model with n_estimators=200 and max_depth=5 are identical with those for the k-NN model just optimized using hyperopt.  There's got to be an explanation for this, other than some kind of lazy data leak, right?","metadata":{}},{"cell_type":"code","source":"# gaussian Naive Bayes Classifier\n\ngnb = GaussianNB()\ngnb.fit(X_train, y_train)\ny_pred = gnb.predict(X_val)\naccuracy = accuracy_score(y_val, y_pred)\nf1 = f1_score(y_val, y_pred)\nra = roc_auc_score(y_val, y_pred)\nitem = [\"Gaussian Naive Bayes\", \"N/A\", f1, ra, accuracy]\nitemdict = dict(zip(results_cols, item))\nresults=results.append(itemdict, ignore_index=True)\nprint(\"Gaussian Naive Bayes\\n\", \"Accuracy:\", accuracy, \"f1:\", f1, \"roc_auc:\", ra)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Multi-Layer Perceptron using Hyperopt\n\nmlp_space={'hidden_layer_sizes': hp.uniform(\"hidden_layer_sizes\", 50, 150),\n       'activation': hp.choice('activation', ['relu', 'tanh', 'logistic']),\n        'learning_rate_init': hp.uniform('learning_rate_init', 0.0001,0.01)\n    }\n\ndef objective(space):\n        mlp = MLPClassifier(\n                    hidden_layer_sizes = mlp_space['hidden_layer_sizes'],\n                    activation = mlp_space['activation'],\n                    learning_rate_init = mlp_space['learning_rate_init'])\n    \n    \n        evaluation = [( X_train, y_train), ( X_val, y_val)]\n    \n        mlp.fit(X_train, y_train)\n        \n        y_pred = mlp.predict(X_val)\n        accuracy = accuracy_score(y_val, y_pred)\n        f1 = f1_score(y_val, y_pred)\n        ra = roc_auc_score(y_val, y_pred)\n        return {'loss': -accuracy, 'accuracy': accuracy, 'f1': f1, 'roc_auc': ra, 'status': STATUS_OK}\n\ntrials = Trials()\n\nbest_hyperparams = hyperopt.fmin(fn = objective,\n                    space = mlp_space,\n                    algo = hyperopt.tpe.suggest,\n                    max_evals = 100,\n                    trials = trials)\n    \nassess = trials.best_trial['result']\n\nitem = [\"Multi-Layer Perceptron (Hyperopt)\", best_hyperparams, assess['f1'], assess['roc_auc'], assess['accuracy']]\nitemdict = dict(zip(results_cols, item))\nresults=results.append(itemdict, ignore_index=True)\nprint(\"Multi-Layer Perceptron (Hyperopt)\\n\", \"Hyperparameters:\", hyperparams, \"\\nAccuracy:\", accuracy, \"f1:\", f1, \"roc_auc:\", ra)    \n    \n\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Well, that failed, and \"TypeError: len of pyll.Apply either undefined or unknown\" isn't producing search results that are immediately useful.  I guess I'll just run MLP with defaults for completeness.","metadata":{}},{"cell_type":"code","source":"\nmlp = MLPClassifier()\nmlp.fit(X_train, y_train)\ny_pred = mlp.predict(X_val)\naccuracy = accuracy_score(y_val, y_pred)\nf1 = f1_score(y_val, y_pred)\nra = roc_auc_score(y_val, y_pred)\nitem = [\"MLP Classifier\", \"defaults\", f1, ra, accuracy]\nitemdict = dict(zip(results_cols, item))\nresults=results.append(itemdict, ignore_index=True)\nprint(\"MLP Classifier\\n\", \"Accuracy:\", accuracy, \"f1:\", f1, \"roc_auc:\", ra)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Interestingly, I'm seeing repeated values for accuracy.  I suppose that with 178 observations in the validation set, this makes sense.","metadata":{}},{"cell_type":"code","source":"# following someone's example I found at \n# https://towardsdatascience.com/top-10-binary-classification-algorithms-a-beginners-guide-feeacbd7a3e2\n# here is a homespun neural net using keras\n\nfrom keras import layers\nfrom keras import models\nfrom keras import optimizers\nfrom keras import losses\nfrom keras import regularizers\nfrom keras import metrics\n\nmodel=models.Sequential()\nmodel.add(layers.Dense(8,kernel_regularizer=regularizers.l2(0.003),activation='relu',input_shape=(6,)))\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(8,kernel_regularizer=regularizers.l2(0.003),activation='relu'))\nmodel.add(layers.Dropout(0.6))\nmodel.add(layers.Dense(1,activation='sigmoid'))\nmodel.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])\nmodel.fit(X_train,y_train,epochs=4,batch_size=512,validation_data=(X_val ,y_val))\nprint(\"score on train: \" + str(model.evaluate(X_train,y_train)[1]))\nprint(\"score on val: \"+ str(model.evaluate(X_val,y_val)[1]))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Wow, that was a hassle to adapt and the results were simply dismal.  Plainly, I'm failing to understand what's going on here and need to read some more about keras in general.  I'm going to make some predictions and submit them as a first step.","metadata":{}},{"cell_type":"code","source":"results","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on these results, and seeing as how accuracy is the metric used by Kaggle here (is it?), I'm going to implement the RF model with n_estimators=120 and max_depth=4.","metadata":{}},{"cell_type":"code","source":"test_data.columns","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data[[\"Sex\", \"Embarked\"]] = test_data[[\"Sex\", \"Embarked\"]].apply(LabelEncoder().fit_transform)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for column in test_data.columns:\n    print(\"variable:\", column, \"NaN count:\", test_data[column].isna().sum())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# there's a NaN for fare, here's a simple fix\nmean_fare = test_data[\"Fare\"].mean()\ntest_data[\"Fare\"].fillna(value=mean_fare, inplace=True)\nprint(\"Mean fare:\", mean_fare)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rfc = RandomForestClassifier(n_estimators=120, max_depth=4, random_state=20)\nrfc.fit(X_train, y_train)\npredict = rfc.predict(test_data[predictor_variables])\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predict})\noutput.to_csv('submission.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}